{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark Mini Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPK3c71Pc2fkZYmtVMguHzZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaddanki001/Spark-Mini-Project/blob/master/Spark_Mini_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYYaJOf2wj9y"
      },
      "source": [
        "### **Hadoop Installation Part**\r\n",
        "\r\n",
        "Hadoop is a Java-based programming framework that supports the processing and storage of extremely large datasets on a cluster of inexpensive machines. It was the first major open source project in the big data playing field and is sponsored by the Apache Software Foundation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc8Axvz3xDJc"
      },
      "source": [
        "### **Pre Installation Steps (Clone from GIT)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvBec3rEwnBM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3579f497-79ea-4584-d113-3253896b30ab"
      },
      "source": [
        "!rm -R Spark-Mini-Project\r\n",
        "!rm spark-3.0.2-bin-hadoop2.7-hive1.2.tgz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'Spark-Mini-Project': No such file or directory\n",
            "rm: cannot remove 'spark-3.0.2-bin-hadoop2.7-hive1.2.tgz': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXLmkb4X-sMW",
        "outputId": "50b96c09-1f23-44ff-ad7b-1e2228186360"
      },
      "source": [
        "!git clone https://github.com/vaddanki001/Spark-Mini-Project"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Spark-Mini-Project'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (6/6), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NL0CJmYw0pt"
      },
      "source": [
        "### **Step 1:Installing Hadoop and Spark**\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fagQHOpw2AB"
      },
      "source": [
        "!wget -q https://www-us.apache.org/dist/spark/spark-3.0.2/spark-3.0.2-bin-hadoop2.7-hive1.2.tgz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-Y5a55m1ddv",
        "outputId": "d7f5d5d8-425c-452d-e730-582b4944b339"
      },
      "source": [
        "!tar xf spark-3.0.2-bin-hadoop2.7-hive1.2.tgz\r\n",
        "\r\n",
        "!pip install -q findspark\r\n",
        "\r\n",
        "!pip install pyspark"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/67/5158f846202d7f012d1c9ca21c3549a58fd3c6707ae8ee823adcaca6473c/pyspark-3.0.2.tar.gz (204.8MB)\n",
            "\u001b[K     |████████████████████████████████| 204.8MB 73kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 20.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.2-py2.py3-none-any.whl size=205186687 sha256=917f883b0f8250495b90f8a3b592d9f2b1dc15a51fbac9ea1f2cc9137164a4a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/09/da/c1f2859bcc86375dc972c5b6af4881b3603269bcc4c9be5d16\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKcT3_KE1g28"
      },
      "source": [
        "### **Step2: install JDK**\r\n",
        "Hadoop/Spark requires that you set the path to Java, either as an environment variable or in the Hadoop configuration file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7YTXkaJ1kIi",
        "outputId": "5185e764-d350-440d-a304-d0e4af9e8d71"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\r\n",
        "#To find the default Java path\r\n",
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/jvm/java-11-openjdk-amd64/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T85j2BfE3SeH"
      },
      "source": [
        "### Step 3: Setting **Java and Spark Home**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P-8KrwB3RLR"
      },
      "source": [
        "import os\r\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\r\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.2-bin-hadoop2.7-hive1.2\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tslbivnx50Sg",
        "outputId": "a20e2e7f-c82a-46ac-f073-57d04ada6265"
      },
      "source": [
        "!echo $SPARK_HOME"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/spark-3.0.2-bin-hadoop2.7-hive1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ijxljo0l7nK5"
      },
      "source": [
        "### Step 4: **Initiate Spark Session**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTyuuGrx6dUm"
      },
      "source": [
        "import findspark\r\n",
        "findspark.init()\r\n",
        "\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\r\n",
        "\r\n",
        "from pyspark import SparkContext\r\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmXAA1_B_rrQ"
      },
      "source": [
        "### Step 5: **Check Datafile exists**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuIe2s8e_sCo",
        "outputId": "cd6ef18c-e7f4-42fc-8394-136b13657fdf"
      },
      "source": [
        "!cat /content/Spark-Mini-Project/data.csv"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1,I,VXIO456XLBB630221,Nissan,Altima,2003,2002-05-08,Initial sales from TechMotors\n",
            "2,I,INU45KIOOPA343980,Mercedes,C300,2015,2014-01-01,Sold from EuroMotors\n",
            "3,A,VXIO456XLBB630221,,,,2014-07-02,Head on collision\n",
            "4,R,VXIO456XLBB630221,,,,2014-08-05,Repair transmission\n",
            "5,I,VOME254OOXW344325,Mercedes,E350,2015,2014-02-01,Sold from Carmax\n",
            "6,R,VOME254OOXW344325,,,,2015-02-06,Wheel allignment service\n",
            "7,R,VXIO456XLBB630221,,,,2015-01-01,Replace right head light\n",
            "8,I,EXOA00341AB123456,Mercedes,SL550,2016,2015-01-01,Sold from AceCars\n",
            "9,A,VOME254OOXW344325,,,,2015-10-01,Side collision\n",
            "10,R,VOME254OOXW344325,,,,2015-09-01,Changed tires\n",
            "11,R,EXOA00341AB123456,,,,2015-05-01,Repair engine\n",
            "12,A,EXOA00341AB123456,,,,2015-05-03,Vehicle rollover\n",
            "13,R,VOME254OOXW344325,,,,2015-09-01,Replace passenger side door\n",
            "14,I,UXIA769ABCC447906,Toyota,Camery,2017,2016-05-08,Initial sales from Carmax\n",
            "15,R,UXIA769ABCC447906,,,,2020-01-02,Initial sales from Carmax\n",
            "16,A,INU45KIOOPA343980,,,,2020-05-01,Side collision\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsnrMv3wugjn"
      },
      "source": [
        "### Step 6: **Read the Datafile**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9CLIrRv_7VP"
      },
      "source": [
        "raw_rdd = sc.textFile(\"/content/Spark-Mini-Project/data.csv\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXzhFlnhuomd"
      },
      "source": [
        "### Step 7: **Split the row delimited by ,**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEZHjujwCUoq",
        "outputId": "cab6f528-924a-454e-a410-b1c21dcbef0e"
      },
      "source": [
        "records_rdd = raw_rdd.map(lambda x : x.split(\",\"))\r\n",
        "\r\n",
        "records_rdd.first()"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1',\n",
              " 'I',\n",
              " 'VXIO456XLBB630221',\n",
              " 'Nissan',\n",
              " 'Altima',\n",
              " '2003',\n",
              " '2002-05-08',\n",
              " 'Initial sales from TechMotors']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTODrDsku8CK"
      },
      "source": [
        "### Step 8: **Map the records by vin number and sort**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6TP3kuADy0j",
        "outputId": "a54f0b98-7dbb-438f-fd0f-c7aa2b20d5e5"
      },
      "source": [
        "sortedRDD = records_rdd.map(lambda line : (line[2], (line) ) ).sortByKey()\r\n",
        "\r\n",
        "sortedRDD.take(5)"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('EXOA00341AB123456',\n",
              "  ['8',\n",
              "   'I',\n",
              "   'EXOA00341AB123456',\n",
              "   'Mercedes',\n",
              "   'SL550',\n",
              "   '2016',\n",
              "   '2015-01-01',\n",
              "   'Sold from AceCars']),\n",
              " ('EXOA00341AB123456',\n",
              "  ['11', 'R', 'EXOA00341AB123456', '', '', '', '2015-05-01', 'Repair engine']),\n",
              " ('EXOA00341AB123456',\n",
              "  ['12',\n",
              "   'A',\n",
              "   'EXOA00341AB123456',\n",
              "   '',\n",
              "   '',\n",
              "   '',\n",
              "   '2015-05-03',\n",
              "   'Vehicle rollover']),\n",
              " ('INU45KIOOPA343980',\n",
              "  ['2',\n",
              "   'I',\n",
              "   'INU45KIOOPA343980',\n",
              "   'Mercedes',\n",
              "   'C300',\n",
              "   '2015',\n",
              "   '2014-01-01',\n",
              "   'Sold from EuroMotors']),\n",
              " ('INU45KIOOPA343980',\n",
              "  ['16',\n",
              "   'A',\n",
              "   'INU45KIOOPA343980',\n",
              "   '',\n",
              "   '',\n",
              "   '',\n",
              "   '2020-05-01',\n",
              "   'Side collision'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gJtlK57u-3l"
      },
      "source": [
        "### Step 9: **Map the records by vin number and filter records with Initial sale**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUqSMH6nUgo7",
        "outputId": "1a7e07e6-62dc-4606-9029-7151fac0a2e1"
      },
      "source": [
        "vinMakeRDD = records_rdd.filter(lambda x: x[1] == 'I')\r\n",
        "vinKV = vinMakeRDD.map(lambda line : (line[2], (line[3] ,line[4] ,line[5]) ) )\r\n",
        "\r\n",
        "vinKV.take(5)\r\n"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('VXIO456XLBB630221', ('Nissan', 'Altima', '2003')),\n",
              " ('INU45KIOOPA343980', ('Mercedes', 'C300', '2015')),\n",
              " ('VOME254OOXW344325', ('Mercedes', 'E350', '2015')),\n",
              " ('EXOA00341AB123456', ('Mercedes', 'SL550', '2016')),\n",
              " ('UXIA769ABCC447906', ('Toyota', 'Camery', '2017'))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbumF9LhvLuc"
      },
      "source": [
        "### Step 10: **Join vinKV rdd and sorted rdd with vin as the key**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoA72TnkcKZe",
        "outputId": "7f8dba97-8ded-4a6d-a88c-1aa7cc1a2684"
      },
      "source": [
        "finalRDD = sortedRDD.join(vinKV)\r\n",
        "\r\n",
        "\r\n",
        "replacedRDD = finalRDD.map(lambda x : [x[1][0][0],x[1][0][1],x[1][0][2],x[1][1][0],x[1][1][1],x[1][1][2],x[1][0][6],x[1][0][7] ])\r\n",
        "# replacedRDD = finalRDD.map(lambda x : x[1][1][0] )\r\n",
        "for i in replacedRDD.take(20): print(i)"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['8', 'I', 'EXOA00341AB123456', 'Mercedes', 'SL550', '2016', '2015-01-01', 'Sold from AceCars']\n",
            "['11', 'R', 'EXOA00341AB123456', 'Mercedes', 'SL550', '2016', '2015-05-01', 'Repair engine']\n",
            "['12', 'A', 'EXOA00341AB123456', 'Mercedes', 'SL550', '2016', '2015-05-03', 'Vehicle rollover']\n",
            "['14', 'I', 'UXIA769ABCC447906', 'Toyota', 'Camery', '2017', '2016-05-08', 'Initial sales from Carmax']\n",
            "['15', 'R', 'UXIA769ABCC447906', 'Toyota', 'Camery', '2017', '2020-01-02', 'Initial sales from Carmax']\n",
            "['5', 'I', 'VOME254OOXW344325', 'Mercedes', 'E350', '2015', '2014-02-01', 'Sold from Carmax']\n",
            "['6', 'R', 'VOME254OOXW344325', 'Mercedes', 'E350', '2015', '2015-02-06', 'Wheel allignment service']\n",
            "['9', 'A', 'VOME254OOXW344325', 'Mercedes', 'E350', '2015', '2015-10-01', 'Side collision']\n",
            "['10', 'R', 'VOME254OOXW344325', 'Mercedes', 'E350', '2015', '2015-09-01', 'Changed tires']\n",
            "['13', 'R', 'VOME254OOXW344325', 'Mercedes', 'E350', '2015', '2015-09-01', 'Replace passenger side door']\n",
            "['2', 'I', 'INU45KIOOPA343980', 'Mercedes', 'C300', '2015', '2014-01-01', 'Sold from EuroMotors']\n",
            "['16', 'A', 'INU45KIOOPA343980', 'Mercedes', 'C300', '2015', '2020-05-01', 'Side collision']\n",
            "['1', 'I', 'VXIO456XLBB630221', 'Nissan', 'Altima', '2003', '2002-05-08', 'Initial sales from TechMotors']\n",
            "['3', 'A', 'VXIO456XLBB630221', 'Nissan', 'Altima', '2003', '2014-07-02', 'Head on collision']\n",
            "['4', 'R', 'VXIO456XLBB630221', 'Nissan', 'Altima', '2003', '2014-08-05', 'Repair transmission']\n",
            "['7', 'R', 'VXIO456XLBB630221', 'Nissan', 'Altima', '2003', '2015-01-01', 'Replace right head light']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e9hseY7vkvU"
      },
      "source": [
        "### Step 11: **Map the records with make and year and reduce by make-year key**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5zjQXiiah2N",
        "outputId": "2d337825-ec51-42c6-b5d7-2e2d676374d4"
      },
      "source": [
        "finalRDD = replacedRDD.filter(lambda x: x[1] != 'I').map(lambda x: (x[3] +'-' +x[5],1)).reduceByKey(lambda x,y : x+y)\r\n",
        "\r\n",
        "finalRDD.take(20)"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Nissan-2003', 3),\n",
              " ('Toyota-2017', 1),\n",
              " ('Mercedes-2015', 5),\n",
              " ('Mercedes-2016', 2)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    }
  ]
}